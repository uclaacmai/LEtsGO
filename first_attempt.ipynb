{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torch import nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegoDataset(Dataset):\n",
    "    \"\"\"Custom dataset class for Lego Minifigure dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_dir, test=False, transform=None, target_transform=None):\n",
    "        \"\"\"init method.\n",
    "        \n",
    "        Keyword arguments:\n",
    "        img_dir -- the path to the root image directory of test and train data\n",
    "        test -- True to load test data (default: False)\n",
    "        transform -- transform to apply to X\n",
    "        target_transform -- transform to apply to y\"\"\"\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.test = test\n",
    "        self.img_dir = os.path.join(img_dir, \"test/\" if test else \"train/\")\n",
    "        self.full_df = None\n",
    "        \n",
    "        # read path names and class names from csv files\n",
    "        meta_df = pd.read_csv(os.path.join(self.img_dir, \"metadata.csv\"))\n",
    "        if self.test:\n",
    "            test_df = pd.read_csv(os.path.join(self.img_dir, \"test.csv\"))\n",
    "            self.full_df = test_df.merge(meta_df, on=\"class_id\")\n",
    "        else:\n",
    "            train_df = pd.read_csv(os.path.join(self.img_dir, \"index.csv\"))\n",
    "            self.full_df = train_df.merge(meta_df, on=\"class_id\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.full_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.full_df.iloc[idx]\n",
    "        image = read_image(os.path.join(self.img_dir, row[\"path\"])).float()\n",
    "        label = row[\"class_id\"] - 1\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load datasets for getting mean and std for standardization\n",
    "def get_mean_std(size=[224, 224]):\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(size)\n",
    "    ])\n",
    "    train_dataset = LegoDataset(\"data/\", test=False, transform=transform)\n",
    "\n",
    "    # get mean and std of dataset\n",
    "    loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
    "    data = next(iter(loader))\n",
    "    return data[0].mean(), data[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep data for real\n",
    "input_size = [64, 64]\n",
    "mean, std = get_mean_std(input_size)\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(input_size),\n",
    "    torchvision.transforms.Normalize(mean, std)\n",
    "])\n",
    "train_dataset = LegoDataset(\"data/\", test=False, transform=transform)\n",
    "test_dataset = LegoDataset(\"data/\", test=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple conv net\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # conv_stack\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(16, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(32, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2))\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # 2 dense layers\n",
    "        self.dense_stack = nn.Sequential(\n",
    "            nn.Linear(1152, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 36)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_stack(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self, optimizer, loss_fn, train_loader, test_loader, n_epochs):\n",
    "        for e in range(n_epochs):\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for X, y in train_loader:\n",
    "                # get preds\n",
    "                preds = self.forward(X)\n",
    "                loss = loss_fn(preds, y)\n",
    "                \n",
    "                # backprop\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # collect stats\n",
    "                max_preds = preds.argmax(1)\n",
    "                total += y.size()[0]\n",
    "                correct += sum(max_preds == y)\n",
    "            test_total = 0\n",
    "            test_correct = 0\n",
    "            \n",
    "            # get test acc\n",
    "            with torch.no_grad():\n",
    "                for X, y in test_loader:\n",
    "                    preds = model(X)\n",
    "                    max_preds = preds.argmax(1)\n",
    "                    test_total += y.size()[0]\n",
    "                    test_correct += sum(max_preds == y)\n",
    "            \n",
    "            train_acc = correct / total\n",
    "            test_acc = test_correct / test_total\n",
    "            # print out stats\n",
    "            print(f\"epoch: {e}\\t train_acc: {train_acc:.3f}\\t test_acc: {test_acc:.3f}\")\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv_stack): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (dense_stack): Sequential(\n",
      "    (0): Linear(in_features=1152, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=36, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# run everything!\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=16)\n",
    "model = ConvNet()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t train_acc: 0.040\t test_acc: 0.068\n",
      "epoch: 1\t train_acc: 0.065\t test_acc: 0.041\n",
      "epoch: 2\t train_acc: 0.128\t test_acc: 0.096\n",
      "epoch: 3\t train_acc: 0.222\t test_acc: 0.123\n",
      "epoch: 4\t train_acc: 0.372\t test_acc: 0.205\n",
      "epoch: 5\t train_acc: 0.537\t test_acc: 0.178\n",
      "epoch: 6\t train_acc: 0.636\t test_acc: 0.178\n",
      "epoch: 7\t train_acc: 0.770\t test_acc: 0.247\n",
      "epoch: 8\t train_acc: 0.852\t test_acc: 0.205\n",
      "epoch: 9\t train_acc: 0.881\t test_acc: 0.233\n",
      "epoch: 10\t train_acc: 0.938\t test_acc: 0.205\n",
      "epoch: 11\t train_acc: 0.960\t test_acc: 0.247\n",
      "epoch: 12\t train_acc: 0.960\t test_acc: 0.219\n",
      "epoch: 13\t train_acc: 0.983\t test_acc: 0.301\n",
      "epoch: 14\t train_acc: 0.994\t test_acc: 0.260\n",
      "epoch: 15\t train_acc: 0.997\t test_acc: 0.247\n",
      "epoch: 16\t train_acc: 0.997\t test_acc: 0.288\n",
      "epoch: 17\t train_acc: 1.000\t test_acc: 0.274\n",
      "epoch: 18\t train_acc: 0.997\t test_acc: 0.274\n",
      "epoch: 19\t train_acc: 0.997\t test_acc: 0.274\n",
      "epoch: 20\t train_acc: 1.000\t test_acc: 0.315\n",
      "epoch: 21\t train_acc: 1.000\t test_acc: 0.274\n",
      "epoch: 22\t train_acc: 1.000\t test_acc: 0.315\n",
      "epoch: 23\t train_acc: 1.000\t test_acc: 0.288\n",
      "epoch: 24\t train_acc: 1.000\t test_acc: 0.260\n",
      "epoch: 25\t train_acc: 1.000\t test_acc: 0.260\n",
      "epoch: 26\t train_acc: 1.000\t test_acc: 0.260\n",
      "epoch: 27\t train_acc: 1.000\t test_acc: 0.260\n",
      "epoch: 28\t train_acc: 1.000\t test_acc: 0.260\n",
      "epoch: 29\t train_acc: 1.000\t test_acc: 0.260\n",
      "epoch: 30\t train_acc: 1.000\t test_acc: 0.260\n",
      "epoch: 31\t train_acc: 1.000\t test_acc: 0.260\n",
      "epoch: 32\t train_acc: 1.000\t test_acc: 0.260\n",
      "epoch: 33\t train_acc: 1.000\t test_acc: 0.260\n",
      "epoch: 34\t train_acc: 1.000\t test_acc: 0.260\n",
      "epoch: 35\t train_acc: 1.000\t test_acc: 0.260\n",
      "epoch: 36\t train_acc: 1.000\t test_acc: 0.260\n",
      "epoch: 37\t train_acc: 1.000\t test_acc: 0.260\n",
      "epoch: 38\t train_acc: 1.000\t test_acc: 0.260\n",
      "epoch: 39\t train_acc: 1.000\t test_acc: 0.260\n",
      "epoch: 40\t train_acc: 1.000\t test_acc: 0.260\n",
      "epoch: 41\t train_acc: 1.000\t test_acc: 0.274\n",
      "epoch: 42\t train_acc: 1.000\t test_acc: 0.274\n",
      "epoch: 43\t train_acc: 1.000\t test_acc: 0.274\n",
      "epoch: 44\t train_acc: 1.000\t test_acc: 0.274\n",
      "epoch: 45\t train_acc: 1.000\t test_acc: 0.274\n",
      "epoch: 46\t train_acc: 1.000\t test_acc: 0.274\n",
      "epoch: 47\t train_acc: 1.000\t test_acc: 0.274\n",
      "epoch: 48\t train_acc: 1.000\t test_acc: 0.274\n",
      "epoch: 49\t train_acc: 1.000\t test_acc: 0.274\n"
     ]
    }
   ],
   "source": [
    "model.train(optimizer, loss_fn, train_loader, test_loader, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# lets try ResNet!\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new transform\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# data aug for training\n",
    "data_aug = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomRotation(45)\n",
    "    ]),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# data loaders\n",
    "train_dataset = LegoDataset(\"data/\", test=False, transform=data_aug)\n",
    "test_dataset = LegoDataset(\"data/\", test=True, transform=preprocess)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t train_acc: 0.060\t test_acc: 0.205\n",
      "epoch: 1\t train_acc: 0.276\t test_acc: 0.110\n",
      "epoch: 2\t train_acc: 0.497\t test_acc: 0.260\n",
      "epoch: 3\t train_acc: 0.619\t test_acc: 0.548\n",
      "epoch: 4\t train_acc: 0.662\t test_acc: 0.479\n",
      "epoch: 5\t train_acc: 0.659\t test_acc: 0.521\n",
      "epoch: 6\t train_acc: 0.679\t test_acc: 0.616\n",
      "epoch: 7\t train_acc: 0.756\t test_acc: 0.630\n",
      "epoch: 8\t train_acc: 0.773\t test_acc: 0.658\n",
      "epoch: 9\t train_acc: 0.778\t test_acc: 0.740\n",
      "epoch: 10\t train_acc: 0.835\t test_acc: 0.685\n",
      "epoch: 11\t train_acc: 0.741\t test_acc: 0.671\n",
      "epoch: 12\t train_acc: 0.750\t test_acc: 0.740\n",
      "epoch: 13\t train_acc: 0.821\t test_acc: 0.589\n",
      "epoch: 14\t train_acc: 0.798\t test_acc: 0.644\n",
      "epoch: 15\t train_acc: 0.804\t test_acc: 0.671\n",
      "epoch: 16\t train_acc: 0.832\t test_acc: 0.685\n",
      "epoch: 17\t train_acc: 0.847\t test_acc: 0.699\n",
      "epoch: 18\t train_acc: 0.815\t test_acc: 0.685\n",
      "epoch: 19\t train_acc: 0.832\t test_acc: 0.630\n",
      "epoch: 20\t train_acc: 0.864\t test_acc: 0.671\n",
      "epoch: 21\t train_acc: 0.864\t test_acc: 0.671\n",
      "epoch: 22\t train_acc: 0.864\t test_acc: 0.753\n",
      "epoch: 23\t train_acc: 0.861\t test_acc: 0.699\n",
      "epoch: 24\t train_acc: 0.841\t test_acc: 0.685\n",
      "epoch: 25\t train_acc: 0.886\t test_acc: 0.616\n",
      "epoch: 26\t train_acc: 0.875\t test_acc: 0.644\n",
      "epoch: 27\t train_acc: 0.855\t test_acc: 0.781\n",
      "epoch: 28\t train_acc: 0.864\t test_acc: 0.753\n",
      "epoch: 29\t train_acc: 0.872\t test_acc: 0.699\n",
      "epoch: 30\t train_acc: 0.852\t test_acc: 0.699\n",
      "epoch: 31\t train_acc: 0.869\t test_acc: 0.767\n",
      "epoch: 32\t train_acc: 0.881\t test_acc: 0.740\n",
      "epoch: 33\t train_acc: 0.886\t test_acc: 0.699\n",
      "epoch: 34\t train_acc: 0.898\t test_acc: 0.767\n",
      "epoch: 35\t train_acc: 0.847\t test_acc: 0.726\n",
      "epoch: 36\t train_acc: 0.892\t test_acc: 0.753\n",
      "epoch: 37\t train_acc: 0.892\t test_acc: 0.699\n",
      "epoch: 38\t train_acc: 0.855\t test_acc: 0.795\n",
      "epoch: 39\t train_acc: 0.892\t test_acc: 0.644\n",
      "epoch: 40\t train_acc: 0.875\t test_acc: 0.740\n",
      "epoch: 41\t train_acc: 0.878\t test_acc: 0.726\n",
      "epoch: 42\t train_acc: 0.884\t test_acc: 0.699\n",
      "epoch: 43\t train_acc: 0.884\t test_acc: 0.712\n",
      "epoch: 44\t train_acc: 0.886\t test_acc: 0.671\n",
      "epoch: 45\t train_acc: 0.886\t test_acc: 0.712\n",
      "epoch: 46\t train_acc: 0.903\t test_acc: 0.712\n",
      "epoch: 47\t train_acc: 0.881\t test_acc: 0.685\n",
      "epoch: 48\t train_acc: 0.889\t test_acc: 0.740\n",
      "epoch: 49\t train_acc: 0.884\t test_acc: 0.753\n",
      "epoch: 50\t train_acc: 0.878\t test_acc: 0.726\n",
      "epoch: 51\t train_acc: 0.920\t test_acc: 0.753\n",
      "epoch: 52\t train_acc: 0.861\t test_acc: 0.740\n",
      "epoch: 53\t train_acc: 0.861\t test_acc: 0.712\n",
      "epoch: 54\t train_acc: 0.906\t test_acc: 0.740\n",
      "epoch: 55\t train_acc: 0.929\t test_acc: 0.767\n",
      "epoch: 56\t train_acc: 0.920\t test_acc: 0.712\n",
      "epoch: 57\t train_acc: 0.881\t test_acc: 0.767\n",
      "epoch: 58\t train_acc: 0.872\t test_acc: 0.726\n",
      "epoch: 59\t train_acc: 0.881\t test_acc: 0.671\n",
      "epoch: 60\t train_acc: 0.901\t test_acc: 0.753\n",
      "epoch: 61\t train_acc: 0.906\t test_acc: 0.795\n",
      "epoch: 62\t train_acc: 0.892\t test_acc: 0.712\n",
      "epoch: 63\t train_acc: 0.886\t test_acc: 0.753\n",
      "epoch: 64\t train_acc: 0.915\t test_acc: 0.740\n",
      "epoch: 65\t train_acc: 0.906\t test_acc: 0.767\n",
      "epoch: 66\t train_acc: 0.906\t test_acc: 0.726\n",
      "epoch: 67\t train_acc: 0.920\t test_acc: 0.767\n",
      "epoch: 68\t train_acc: 0.923\t test_acc: 0.753\n",
      "epoch: 69\t train_acc: 0.935\t test_acc: 0.712\n",
      "epoch: 70\t train_acc: 0.923\t test_acc: 0.740\n",
      "epoch: 71\t train_acc: 0.920\t test_acc: 0.767\n",
      "epoch: 72\t train_acc: 0.903\t test_acc: 0.753\n",
      "epoch: 73\t train_acc: 0.901\t test_acc: 0.753\n",
      "epoch: 74\t train_acc: 0.915\t test_acc: 0.753\n",
      "epoch: 75\t train_acc: 0.906\t test_acc: 0.753\n",
      "epoch: 76\t train_acc: 0.938\t test_acc: 0.740\n",
      "epoch: 77\t train_acc: 0.915\t test_acc: 0.767\n",
      "epoch: 78\t train_acc: 0.929\t test_acc: 0.726\n",
      "epoch: 79\t train_acc: 0.915\t test_acc: 0.767\n",
      "epoch: 80\t train_acc: 0.929\t test_acc: 0.767\n",
      "epoch: 81\t train_acc: 0.926\t test_acc: 0.753\n",
      "epoch: 82\t train_acc: 0.915\t test_acc: 0.726\n",
      "epoch: 83\t train_acc: 0.938\t test_acc: 0.726\n",
      "epoch: 84\t train_acc: 0.923\t test_acc: 0.753\n",
      "epoch: 85\t train_acc: 0.915\t test_acc: 0.753\n",
      "epoch: 86\t train_acc: 0.920\t test_acc: 0.740\n",
      "epoch: 87\t train_acc: 0.901\t test_acc: 0.781\n",
      "epoch: 88\t train_acc: 0.909\t test_acc: 0.767\n",
      "epoch: 89\t train_acc: 0.892\t test_acc: 0.753\n",
      "epoch: 90\t train_acc: 0.920\t test_acc: 0.781\n"
     ]
    }
   ],
   "source": [
    "# disable grad for all parameters\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# replace fc layer\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, out_features=36, bias=True)\n",
    "\n",
    "# set up params to update\n",
    "params_to_update = resnet.fc.parameters()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(params_to_update, lr=0.01, momentum=0.9)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, n_epochs)\n",
    "\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    train_total = 0\n",
    "    train_correct = 0\n",
    "    test_total = 0\n",
    "    test_correct = 0\n",
    "    \n",
    "    resnet.train()\n",
    "    for X, y in train_loader:\n",
    "        # forward pass\n",
    "        preds = resnet.forward(X)\n",
    "        loss = loss_fn(preds, y)\n",
    "        \n",
    "        # backward pass\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        # collect stats\n",
    "        preds_max = preds.argmax(1)\n",
    "        train_total += y.size()[0]\n",
    "        train_correct += sum(preds_max == y)\n",
    "    \n",
    "    # update scheduler\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    train_acc = train_correct / train_total\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    resnet.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # forward pass\n",
    "            preds = resnet.forward(X)\n",
    "\n",
    "            # collect stats\n",
    "            preds_max = preds.argmax(1)\n",
    "            test_total += y.size()[0]\n",
    "            test_correct += sum(preds_max == y)\n",
    "    \n",
    "    test_acc = test_correct / test_total\n",
    "    test_accs.append(test_acc)\n",
    "    print(f\"epoch: {e}\\t train_acc: {train_acc:.3f}\\t test_acc: {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "xs = [i for i in range(n_epochs)]\n",
    "\n",
    "plt.plot(xs, train_accs, color='orange', label=\"train_acc\")\n",
    "plt.plot(xs, test_accs, color='blue', label='test_acc')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('test and train acc vs epochs')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
